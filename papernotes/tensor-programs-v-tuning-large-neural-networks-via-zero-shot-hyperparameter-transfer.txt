[arXiv:2203.03466](https://arxiv.org/abs/2203.03466)

[Reproduction](https://github.com/johnma2006/candle/tree/main?tab=readme-ov-file)
  
<img src="papernotes/figures/tensor-programs-v-tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer-title.jpg" width="400" />
<img src="papernotes/figures/tensor-programs-v-tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer-abstract.jpg" width="400" />

#### Intuition

- Under μParameterization, networks of increasing width **converge** to a limit they call the **feature-learning limit** for all training steps (different from the NKT \"lazy\" limit, which does not really describe modern practical networks)
- Therefore, given sufficiently large width, since convergence implies the functional and training dynamics are independent of the width, then the **optimal hyperparameters are also independent of the width**, which allows for hyperparameter transfer from small to large models.

#### Why does it converge?

μParameterization was designed with the following goals [3]
1. Every (pre)activation vector in a network should have Θ(1)-sized coordinates
2. Network output should be O(1).
3. All parameters should be updated as much as possible (in terms of scaling in width) without leading to divergence

The mechanical details of μP achieves these goals are explained in Appendix C [2] and Appendix J [1].

#### References:

- [1] [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/pdf/2203.03466.pdf), Yang et al. 2022
- [2] [Feature Learning in Infinite-Width Neural Networks](https://arxiv.org/abs/2011.14522) Yang and Hu, 2020
- [3] https://github.com/microsoft/mup/tree/main
- [4] https://www.microsoft.com/en-us/research/blog/%C2%B5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/
