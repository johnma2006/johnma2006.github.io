[arXiv:2211.03540](https://arxiv.org/abs/2211.03540.)
  
<img src="papernotes/figures/measuring-progress-on-scalable-oversight-for-large-language-models-title.jpg" width="400" />
<img src="papernotes/figures/measuring-progress-on-scalable-oversight-for-large-language-models-abstract.jpg" width="400" />

#### Goal

- Evaluate scalable oversight techniques with present-day models
- Intuition:
	- Learning about human values is not unlike learning about other subjects
	- We can frame "learning about human values" as a capabilities question

#### Methodology

- <img src="papernotes/figures/measuring-progress-on-scalable-oversight-for-large-language-models-1.jpg" width="400" />
- Similar to weak-to-strong generalization; can weaker humans supervise stronger AI systems to elicit capabilities?
- Choose task where non-expert humans < models < expert humans
	- The non-experts try to align the model to perform the task
	- The experts then evaluate the model
- Example:
	- Consider the task of eliciting medical advice from a LM.
	- While LMs have serious limitations that prevent us from actually deploying them for medical advice, it is reasonable to expect that they could be helpful in some cases if they were well aligned: They have memorized large swaths of internet and book texts, including far more medical research than any one clinician.
		- However, they have also memorized large swaths of inaccurate, dated, or debunked research
		- So by default, we should not expect their responses to be reliably aligned with our goals.
	- Can a group of non-clinicians prompt or train a language model to give only appropriate advice, without at any point involving any clinicians or consulting the medical literature? 
	- One solution might be: prompt a model with a diverse array of prompts and strategies, and accept only answers that the model gives consistently on the basis of consistent and reasonable-sounding evidence
	- Any technique that could address such a challenge with high reliability would likely represent important progress on scalable oversight.

#### Results

- Human participants who interact with an unreliable LLM dialog assistant through chat substantially outperform both the model alone and their own unaided performance.