[https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/)

- Trained on the same dataset for long enough, pretty much every model with enough weights and compute converges to the same loss
	- Sufficiently large diffusion conv-unets produce the same images as ViT generators. 
	- AR sampling produces the same images as diffusion.
- Everything is a means to an end in efficiently using compute to approximating the dataset
- 'Then, when you refer to “Lambda”, “ChatGPT”, “Bard”, or “Claude” then, it’s not the model weights that you are referring to. It’s the dataset.'

