[https://www.lesswrong.com/posts/xhKr5KtvdJRssMeJ3/anthropic-s-core-views-on-ai-safety?commentId=Chwh6AqqaktNZ2MAN](https://www.lesswrong.com/posts/xhKr5KtvdJRssMeJ3/anthropic-s-core-views-on-ai-safety?commentId=Chwh6AqqaktNZ2MAN)

#### Anthropic's Policy Team

Our policy team does three main things

* (1) Proactively educating policymakers about scaling trends of AI systems and their relation to safey
* (2) Pushing a few specific things we care about
    * Evals/measures for AI safety currently aren't very good yet, and so we've spent a lot of time with NIST's "Risk Management Framework" to create more useful policy institutions. NIST is one of the best institutions to take policy insights and (a) standardize some of them (b) circulate insights across governments
* (3) Responding to interest. An increasing amount of work is reactive, meaning policymakers reach out and ask for our thoughts
    * Aim to give impartial, technically informed advice, including advice that isn't necessarily favourable to Anthropic
    * We do this because (a) we think we're well positioned to give policymakers good info and (b) **building influence**: as stakes get higher, policymakers will put higher weight on the advice of labs who "showed up" before it was strategic to do so
