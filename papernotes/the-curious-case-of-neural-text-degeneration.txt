[arXiv:1904.09751](https://arxiv.org/abs/1904.09751)
  
<img src="papernotes/figures/the-curious-case-of-neural-text-degeneration-title.jpg" width="400" />
<img src="papernotes/figures/the-curious-case-of-neural-text-degeneration-abstract.jpg" width="400" />

- Decoding strategies that optimize for outputs with high logprob, such as beam search, lead to text that is degenerate, repetitive, and generic
	- This may seem counter-intuitive, as one would expect good models would assign higher prob to more human-like text. This generally is true, but the highest scores are often given to texts that are repetitive and generic.
		- Why: by evaluating the perplexity of generated text, we see that text generated by maximization is too probable (when compared to the human distribution), indicating a lack of diversity 
		- <img src="papernotes/figures/the-curious-case-of-neural-text-degeneration-1.jpg" width="400" />
	- On the other hand, pure sampling produces text that has too low logprob, corresponding to lower generation quality
		- The paper attributes this to the "unreliable tail", which are tens of thousands of low-prob tokens that are overrepresented in the aggregate
	- The per-token probability of human-generated natural text is, on average, much lower than text generated by beam search
		- Natural language rarely remains in a high probability zone for multiple time steps, instead veering into lower-prob but more informative tokens.
		- Natural language also doesn't fall into repetition loops, which models often assign high prob to.
- Top-p sampling truncates all tokens outside the top-p = e.g. 0.95 cumulative prob
